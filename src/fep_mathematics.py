#!/usr/bin/env python3 """ FREE ENERGY PRINCIPLE - MATHEMATICAL FOUNDATION ================================================= Proper implementation of Free Energy Principle mathematics for cognitive architectures. This module implements the core mathematical framework: - Variational Free Energy calculation: F = E_q[log q(z) - log p(x,z)] - Bayesian belief updating through gradient descent - Hierarchical generative models - Prediction error minimization Based on: - Friston, K. (2010). The free-energy principle: a unified brain theory? - Parr, T., Pezzulo, G., & Friston, K. J. (2022). Active Inference: The Free Energy Principle in Mind, Brain, and Behavior """ import numpy as np import torch import torch.nn as nn import torch.nn.functional as F from scipy.stats import multivariate_normal from typing import Dict, List, Tuple, Optional, Union import logging logger = logging.getLogger(__name__) class GenerativeModel(nn.Module): """ Hierarchical generative model p(x, z) for FEP implementation. This implements the generative model that describes how observations (x) are generated from latent states (z) according to the system's beliefs. """ def __init__(self, observation_dim: int, latent_dim: int, hidden_dims: List[int] = [128, 64]): super().__init__() self.observation_dim = observation_dim self.latent_dim = latent_dim # Prior network p(z) self.prior_mean = nn.Parameter(torch.zeros(latent_dim)) self.prior_logvar = nn.Parameter(torch.zeros(latent_dim)) # Likelihood network p(x|z) layers = [] prev_dim = latent_dim for hidden_dim in hidden_dims: layers.extend([ nn.Linear(prev_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.1) ]) prev_dim = hidden_dim layers.append(nn.Linear(prev_dim, observation_dim * 2)) # mean and logvar self.likelihood_network = nn.Sequential(*layers) def sample_prior(self, batch_size: int) -> torch.Tensor: """Sample from prior p(z).""" eps = torch.randn(batch_size, self.latent_dim) return self.prior_mean + torch.exp(0.5 * self.prior_logvar) * eps def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: """ Forward pass: compute p(x|z). Returns: (mean, logvar) of observation distribution """ output = self.likelihood_network(z) mean = output[:, :self.observation_dim] logvar = output[:, self.observation_dim:] return mean, logvar def log_prob_prior(self, z: torch.Tensor) -> torch.Tensor: """Compute log p(z) - prior probability.""" return -0.5 * torch.sum( (z - self.prior_mean).pow(2) / torch.exp(self.prior_logvar) + self.prior_logvar + np.log(2 * np.pi), dim=-1 ) def log_prob_likelihood(self, x: torch.Tensor, z: torch.Tensor) -> torch.Tensor: """Compute log p(x|z) - likelihood.""" mean, logvar = self.forward(z) return -0.5 * torch.sum( (x - mean).pow(2) / torch.exp(logvar) + logvar + np.log(2 * np.pi), dim=-1 ) class VariationalPosterior(nn.Module): """ Variational posterior q(z|x) - the system's beliefs about latent states. This implements the recognition model that infers latent states from observations. """ def __init__(self, observation_dim: int, latent_dim: int, hidden_dims: List[int] = [128, 64]): super().__init__() self.latent_dim = latent_dim # Recognition network q(z|x) layers = [] prev_dim = observation_dim for hidden_dim in hidden_dims: layers.extend([ nn.Linear(prev_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.1) ]) prev_dim = hidden_dim layers.append(nn.Linear(prev_dim, latent_dim * 2)) # mean and logvar self.recognition_network = nn.Sequential(*layers) def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: """ Infer posterior parameters q(z|x). Returns: (mean, logvar) of posterior distribution """ output = self.recognition_network(x) mean = output[:, :self.latent_dim] logvar = output[:, self.latent_dim:] return mean, logvar def sample(self, x: torch.Tensor, n_samples: int = 1) -> torch.Tensor: """Sample from posterior q(z|x).""" mean, logvar = self.forward(x) if n_samples > 1: mean = mean.unsqueeze(1).repeat(1, n_samples, 1) logvar = logvar.unsqueeze(1).repeat(1, n_samples, 1) eps = torch.randn_like(mean) return mean + torch.exp(0.5 * logvar) * eps def log_prob(self, z: torch.Tensor, x: torch.Tensor) -> torch.Tensor: """Compute log q(z|x).""" mean, logvar = self.forward(x) return -0.5 * torch.sum( (z - mean).pow(2) / torch.exp(logvar) + logvar + np.log(2 * np.pi), dim=-1 ) class FreeEnergyCalculator: """ Core Free Energy Principle calculator. Implements the fundamental FEP equation: F = E_q[log q(z|x) - log p(x,z)] Where: - F is the variational free energy (surprise + complexity) - q(z|x) is the variational posterior (beliefs) - p(x,z) is the generative model (world model) """ def __init__(self, generative_model: GenerativeModel, posterior: VariationalPosterior): self.generative_model = generative_model self.posterior = posterior def compute_free_energy(self, observations: torch.Tensor, n_samples: int = 10) -> Dict[str, torch.Tensor]: """ Compute variational free energy F = E_q[log q(z|x) - log p(x,z)]. Args: observations: Input observations x n_samples: Number of Monte Carlo samples for expectation Returns: Dictionary containing: - free_energy: Total variational free energy - reconstruction_error: -E_q[log p(x|z)] (surprise) - kl_divergence: KL[q(z|x) || p(z)] (complexity) - log_evidence: log p(x) approximation """ batch_size = observations.shape[0] # Sample from posterior q(z|x) z_samples = self.posterior.sample(observations, n_samples) # [batch, n_samples, latent_dim] if n_samples > 1: # Reshape for batch processing z_flat = z_samples.view(-1, z_samples.shape[-1]) x_expanded = observations.unsqueeze(1).repeat(1, n_samples, 1).view(-1, observations.shape[-1]) else: z_flat = z_samples.squeeze(1) x_expanded = observations # Compute log probabilities log_q_z_given_x = self.posterior.log_prob(z_flat, x_expanded) # log q(z|x) log_p_z = self.generative_model.log_prob_prior(z_flat) # log p(z) log_p_x_given_z = self.generative_model.log_prob_likelihood(x_expanded, z_flat) # log p(x|z) if n_samples > 1: # Reshape back and average over samples log_q_z_given_x = log_q_z_given_x.view(batch_size, n_samples).mean(dim=1) log_p_z = log_p_z.view(batch_size, n_samples).mean(dim=1) log_p_x_given_z = log_p_x_given_z.view(batch_size, n_samples).mean(dim=1) # Compute components reconstruction_error = -log_p_x_given_z # Surprise (negative log-likelihood) kl_divergence = log_q_z_given_x - log_p_z # Complexity (KL divergence) free_energy = reconstruction_error + kl_divergence # Total free energy log_evidence = -free_energy # Approximate log evidence return { 'free_energy': free_energy, 'reconstruction_error': reconstruction_error, 'kl_divergence': kl_divergence, 'log_evidence': log_evidence, 'posterior_entropy': -log_q_z_given_x, 'prior_surprise': -log_p_z } def compute_prediction_error(self, observations: torch.Tensor, predictions: torch.Tensor) -> torch.Tensor: """ Compute prediction error for active inference. This measures how well the system's predictions match observations. High prediction error indicates surprise and drives learning. """ return F.mse_loss(predictions, observations, reduction='none').mean(dim=-1) def update_beliefs(self, observations: torch.Tensor, learning_rate: float = 0.01) -> Dict[str, float]: """ Update beliefs by minimizing free energy through gradient descent. This implements the core FEP learning mechanism where the system updates its internal models to minimize surprise. """ # Enable gradients self.generative_model.train() self.posterior.train() # Compute free energy fe_components = self.compute_free_energy(observations) loss = fe_components['free_energy'].mean() # Gradient descent step loss.backward() # Manual gradient descent (for demonstration) with torch.no_grad(): for param in self.generative_model.parameters(): if param.grad is not None: param.data -= learning_rate * param.grad param.grad.zero_() for param in self.posterior.parameters(): if param.grad is not None: param.data -= learning_rate * param.grad param.grad.zero_() return { 'free_energy': loss.item(), 'reconstruction_error': fe_components['reconstruction_error'].mean().item(), 'kl_divergence': fe_components['kl_divergence'].mean().item() } class HierarchicalFEPSystem: """ Hierarchical Free Energy Principle system for cognitive architectures. Implements multi-level belief hierarchies where higher levels provide priors for lower levels, enabling hierarchical inference and learning. """ def __init__(self, observation_dim: int, latent_dims: List[int] = [64, 32, 16], hidden_dims: List[int] = [128, 64]): """ Initialize hierarchical FEP system. Args: observation_dim: Dimensionality of observations latent_dims: Dimensions of latent states at each hierarchy level hidden_dims: Hidden layer dimensions for networks """ self.levels = len(latent_dims) self.latent_dims = latent_dims # Create generative models and posteriors for each level self.generative_models = [] self.posteriors = [] self.fe_calculators = [] for i in range(self.levels): if i == 0: # Bottom level: observations -> latent input_dim = observation_dim else: # Higher levels: lower latent -> higher latent input_dim = latent_dims[i-1] gen_model = GenerativeModel(input_dim, latent_dims[i], hidden_dims) posterior = VariationalPosterior(input_dim, latent_dims[i], hidden_dims) fe_calc = FreeEnergyCalculator(gen_model, posterior) self.generative_models.append(gen_model) self.posteriors.append(posterior) self.fe_calculators.append(fe_calc) def hierarchical_inference(self, observations: torch.Tensor) -> List[Dict[str, torch.Tensor]]: """ Perform hierarchical inference through all levels. Returns free energy components at each hierarchical level. """ results = [] current_input = observations for level in range(self.levels): fe_components = self.fe_calculators[level].compute_free_energy(current_input) results.append(fe_components) # For next level, use posterior mean as input if level < self.levels - 1: posterior_mean, _ = self.posteriors[level].forward(current_input) current_input = posterior_mean return results def compute_total_free_energy(self, observations: torch.Tensor) -> torch.Tensor: """Compute total free energy across all hierarchical levels.""" hierarchical_results = self.hierarchical_inference(observations) total_fe = torch.zeros(observations.shape[0]) for level_results in hierarchical_results: total_fe += level_results['free_energy'] return total_fe def create_fep_system(observation_dim: int, latent_dim: int = 32, hierarchical: bool = True) -> Union[FreeEnergyCalculator, HierarchicalFEPSystem]: """ Factory function to create FEP systems. Args: observation_dim: Dimensionality of input observations latent_dim: Dimensionality of latent representations hierarchical: Whether to create hierarchical system Returns: FEP system ready for cognitive architecture integration """ if hierarchical: return HierarchicalFEPSystem( observation_dim=observation_dim, latent_dims=[latent_dim, latent_dim // 2, latent_dim // 4] ) else: gen_model = GenerativeModel(observation_dim, latent_dim) posterior = VariationalPosterior(observation_dim, latent_dim) return FreeEnergyCalculator(gen_model, posterior) # Example usage and validation if __name__ == "__main__": # Test the FEP mathematics implementation print(" Testing Free Energy Principle Implementation") print("=" * 50) # Create test data batch_size, obs_dim, latent_dim = 32, 100, 16 observations = torch.randn(batch_size, obs_dim) # Test simple FEP system print("1. Testing Simple FEP System...") fep_system = create_fep_system(obs_dim, latent_dim, hierarchical=False) fe_components = fep_system.compute_free_energy(observations) print(f" Free Energy: {fe_components['free_energy'].mean().item():.4f}") print(f" Reconstruction Error: {fe_components['reconstruction_error'].mean().item():.4f}") print(f" KL Divergence: {fe_components['kl_divergence'].mean().item():.4f}") # Test hierarchical system print("\n2. Testing Hierarchical FEP System...") hierarchical_fep = create_fep_system(obs_dim, latent_dim, hierarchical=True) total_fe = hierarchical_fep.compute_total_free_energy(observations) print(f" Total Hierarchical Free Energy: {total_fe.mean().item():.4f}") # Test belief updating print("\n3. Testing Belief Updates...") initial_fe = fe_components['free_energy'].mean().item() update_results = fep_system.update_beliefs(observations, learning_rate=0.01) final_fe = update_results['free_energy'] print(f" Initial FE: {initial_fe:.4f}") print(f" Final FE: {final_fe:.4f}") print(f" FE Reduction: {initial_fe - final_fe:.4f}") print("\n FEP Mathematics Implementation Complete!") print("Ready for integration with cognitive architectures.")